- We're playing with 3000 normal flows for training, 800 normal + 400 anomalies for testing (total = 1200 test).
- Goal: Spot anomalies. Ideally spot *all* the anomalies. Missing one = bad.

MODELS WE'RE LOOKING AT:
1. Flow_Aware_LSH (our new method!)
2. Isolation Forest
3. One-Class SVM

I ran 20 (then reduced to 10 since it seemed like overkill) experiments across different seeds.
Each run produced stats- Accuracy, Precision, Recall, F1, ROC, etc. across these models.

METRICS TO CARE ABOUT (and why):
- Accuracy: Meh for imbalanced data. Doesn't tell you if anomalies are getting caught.
- Precision: How many flagged flows were actually anomalous. Important, but secondary.
- Recall: CRITICAL. How many anomalies were caught. High recall = good.
- F1: Balance of precision/recall. Good summary.
- ROC AUC: Probability a random anomaly ranks higher than a random normal. Useful, but tricky with thresholds.
- Time: We care how fast it trains/tests.
- Memory: Helpful for low-resource settings.

ðŸ§  INSIGHTS ON METRICS:

- Binary quantization and LSH work super well because they're lightweight and keep locality (similar flows map close).
- Because LSH is tuned for Hamming distance, it's great for quick comparisons in BPF maps or switch memory.
- Precision is low because LSH is high-recall: it catches lots of anomalies, even borderline ones. But that's the point.
- Compared to Isolation Forest and SVM:
   - LSH catches more anomalies (higher recall, sometimes 97%+)
   - SVM has better F1 but misses a few anomalies
   - Isolation Forest is accurate overall, but misses subtle stuff (low recall)

---

ðŸŒ Setup stuff:

* Dataset = UNSW-NB15 â†’ mix of normal and malicious network flows.
* Using 3000 normal flows to train the models, testing on 1200 (800 normal + 400 anomalies).
* Custom method = Flow_Aware_LSH: does binary quantization + hashes the flow into LSH buckets.
* Learning quantization for 9 features (basically compressing numeric values into binary-like sketches).
* Embeddings from flows â†’ used to check similarity via Hamming distance.
* Anomalies = flows far away from expected clusters (set threshold via percentile of distances).
* Threshold was usually around 26.0 distance.

ðŸ“ˆ Performance Summary (from 20 runs + extra 10 runs):

Flow_Aware_LSH:

* Recall king ðŸ† â†’ consistently ~87% recall, sometimes 97%+
* F1 is decent (~0.57), but Accuracy is lower (~0.56 avg), means a lot of false positives
* ROC AUC around 0.73 â†’ so it can discriminate decently
* Precision is low (~0.42), but thatâ€™s expected since it catches a lot of anomalies (many false positives)
* Very stable across seeds (stddevs are low)

Isolation Forest:

* Best overall accuracy (avg ~0.70), so it's good for catching normal flows
* But lower recall (~40%) â†’ misses a lot of anomalies
* F1 is okay-ish (~0.46)
* Pretty stable and fast, but doesnâ€™t react well to subtle anomalies

One-Class SVM:

* Balance between the two
* Accuracy ~0.70
* Precision & recall around 0.56 / 0.46
* F1 score ~0.50
* More consistent than Isolation Forest, but still not as aggressive in anomaly detection as LSH

ðŸ§  What Iâ€™m taking away:

* Flow_Aware_LSH is perfect when false negatives are worse than false positives â†’ like in network intrusion where missing an attack is worse than sounding a false alarm.
* It's really good at finding weird stuff in the data (super high recall), but not great at telling you whatâ€™s actually normal.
* Trade-off: high recall = more noisy predictions, and thatâ€™s fine if you're doing alert-based systems where human inspection follows.
* Binary quantization + LSH = very lightweight, interpretable, and surprisingly solid for sketch-based detection.
* Training + inference is super fast (~4â€“5s train, ~2s test), very deployable.

ðŸ”® Ideas for future me:

* Try hybrid: use LSH to flag anomalies, then run Isolation Forest/SVM on the flagged ones for a second opinion.
* Maybe dynamically learn threshold instead of fixed percentile?
* Can try learning quantization parameters online or using reinforcement signal.
* Explore multi-bit quantization (not strictly binary) or quantization-aware embeddings.

ðŸ” TL;DR:

> Flow_Aware_LSH = high-recall, low-precision anomaly hunter ðŸ•µï¸
> Isolation Forest = stable but misses subtle stuff ðŸªµ
> One-Class SVM = decent middle ground ðŸ¤·â€â™€ï¸
> LSH feels like a good pre-filter in a pipeline.
> And wow it's fast. Binary ftw.