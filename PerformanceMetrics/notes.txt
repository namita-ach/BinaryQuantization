I ran 20 (then reduced to 10 since it seemed like overkill) experiments across different seeds.
Each run produced stats- Accuracy, Precision, Recall, F1, ROC, etc. across these models.

METRICS TO CARE ABOUT (and why):
- Accuracy: Meh for imbalanced data. Doesn't tell you if anomalies are getting caught.
- Precision: How many flagged flows were actually anomalous. Important, but secondary.
- Recall: CRITICAL. How many anomalies were caught. High recall = good.
- F1: Balance of precision/recall. Good summary.
- ROC AUC: Probability a random anomaly ranks higher than a random normal. Useful, but tricky with thresholds.
- Time: We care how fast it trains/tests.
- Memory: Helpful for low-resource settings.

INSIGHTS ON METRICS:
- Binary quantization and LSH work super well because they're lightweight and keep locality (similar flows map close).
- Because LSH is tuned for Hamming distance, it's great for quick comparisons in BPF maps or switch memory.
- Precision is low because LSH is high-recall: it catches lots of anomalies, even borderline ones. But that's the point.
- Compared to Isolation Forest and SVM:
   - LSH catches more anomalies (higher recall, sometimes 97%+)
   - SVM has better F1 but misses a few anomalies
   - Isolation Forest is accurate overall, but misses subtle stuff (low recall)

Performance Summary

Model              | Accuracy         | Precision        | Recall           | F1 Score         | ROC AUC
-------------------|------------------|------------------|------------------|------------------|------------------
Flow-Aware LSH     | 56.42% ± 4.38%   | 42.73% ± 2.61%   | 87.27% ± 10.12%  | 57.09% ± 3.02%   | 73.53% ± 4.23%
Isolation Forest   | 70.01% ± 3.72%   | 55.85% ± 7.74%   | 40.52% ± 11.72%  | 46.62% ± 10.68%  | 69.88% ± 1.79%
One-Class SVM      | 70.50% ± 2.68%   | 56.67% ± 4.31%   | 46.53% ± 8.25%   | 50.95% ± 6.57%   | 66.79% ± 3.27%


Flow_Aware_LSH:

* Recall: consistently ~87% recall, sometimes 97%+
* F1 is decent (~0.57), but Accuracy is lower (~0.56 avg), means a lot of false positives
* ROC AUC around 0.73: so it can discriminate decently
* Precision is low (~0.42), but that’s expected since it catches a lot of anomalies (many false positives)
* Very stable across seeds (stddevs are low)

Isolation Forest:

* Best overall accuracy (avg ~0.70), so it's good for catching normal flows
* But lower recall (~40%): misses a lot of anomalies
* F1 is okay-ish (~0.46)
* Pretty stable and fast, but doesn’t react well to subtle anomalies

One-Class SVM:

* Balance between the two
* Accuracy ~0.70
* Precision & recall around 0.56 / 0.46
* F1 score ~0.50
* More consistent than Isolation Forest, but still not as aggressive in anomaly detection as LSH

What I’m taking away:
* Flow_Aware_LSH is perfect when false negatives are worse than false positives: like in network intrusion where missing an attack is worse than sounding a false alarm.
* It's really good at finding weird stuff in the data (super high recall), but not great at telling you what’s actually normal.
* Trade-off: high recall = more noisy predictions, and that’s fine if you're doing alert-based systems where human inspection follows.
* Binary quantization + LSH = very lightweight, interpretable, and surprisingly solid for sketch-based detection.
* Training + inference is super fast (~4-5s train, ~2s test), very deployable.

Ideas for future me:

* Try hybrid: use LSH to flag anomalies, then run Isolation Forest/SVM on the flagged ones for a second opinion.
* Maybe dynamically learn threshold instead of fixed percentile?
* Can try learning quantization parameters online or using reinforcement signal.
* Explore multi-bit quantization (not strictly binary) or quantization-aware embeddings.

TL;DR:
> Flow_Aware_LSH = high-recall, low-precision anomaly hunter
> Isolation Forest = stable but misses subtle stuff
> One-Class SVM = decent middle ground
> LSH feels like a good pre-filter in a pipeline.